# -*- coding: utf-8 -*-
"""prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1390tiLrT85pA_ppjxeKUGM9cIkK83WFr
"""

import pandas as pd
import numpy as np
from ast import literal_eval
from sklearn_crfsuite import CRF
from sklearn.model_selection import GridSearchCV
import joblib

import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.stem import PorterStemmer

# Download required NLTK data (only need to do this once)
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt')

data = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
df = data.copy()

# Convert string representations of lists to actual lists
df['Sentence'] = df['Sentence'].apply(literal_eval)
df['NER Tag'] = df['NER Tag'].apply(literal_eval)

test['Sentence'] = test['Sentence'].apply(literal_eval)

# Get all unique tags
tags = set(tag for tags in df['NER Tag'] for tag in tags)


def word2features(sent, i):
    word = sent[i]
    pos_tags = nltk.pos_tag(sent)
    word_pos = pos_tags[i][1]
    stem = PorterStemmer().stem(word.lower())

    features = {
        # Current word features
        'word': word.lower(),
        'stem': stem,
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'word.length': len(word),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'word[:3]': word[:3],
        'word[:2]': word[:2],

        # POS features
        'pos': word_pos,
        'pos[:2]': word_pos[:2],

        # Shape features
        'word.shape': ''.join(['X' if c.isupper() else 'x' if c.islower()
                             else 'd' if c.isdigit() else c for c in word]),

        # Context features
        'pos-1': pos_tags[i-1][1] if i > 0 else '<START>',
        'pos+1': pos_tags[i+1][1] if i < len(sent)-1 else '<END>',
        'word-1': sent[i-1].lower() if i > 0 else '<START>',
        'word+1': sent[i+1].lower() if i < len(sent)-1 else '<END>',
    }

    # Add prefix/suffix patterns
    for n in range(1, 4):
        if len(word) >= n:
            features[f'prefix_{n}'] = word[:n]
            features[f'suffix_{n}'] = word[-n:]



    return features

def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]



# Load the saved model
loaded_model = joblib.load('crf_model.joblib')

# Prepare your test data (same feature extraction as before)
X_test = [sent2features(s) for s in test['Sentence']]

# Make predictions
y_pred = loaded_model.predict(X_test)

# Create submission file
submission = pd.DataFrame({
    'id': test['id'],
    'NER Tag': [str(pred) for pred in y_pred]
})
submission.to_csv('submission.csv', index=False)
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install pymongo[srv]\n",
        "!pip3 install dnspython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT-o3-Hsk2G_",
        "outputId": "2ec43ebb-d221-4826-ad89-18823054b23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dnspython\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn_crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-_Wjrt3k6AR",
        "outputId": "65341322-b3c1-45ac-f7bf-88a50e622877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.6.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grmkwtf6kKU-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string representations of lists to actual lists\n",
        "data = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "df = data.copy()\n",
        "\n",
        "df['Sentence'] = df['Sentence'].apply(literal_eval)\n",
        "df['NER Tag'] = df['NER Tag'].apply(literal_eval)\n",
        "\n",
        "test['Sentence'] = test['Sentence'].apply(literal_eval)\n",
        "\n",
        "# Get all unique tags\n",
        "tags = set(tag for tags in df['NER Tag'] for tag in tags)\n",
        "print(\"Unique tags:\", tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfdm036MkNiq",
        "outputId": "15cedc01-3a21-415f-b981-ca3704144170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tags: {'B-org', 'I-per', 'I-tim', 'I-eve', 'B-geo', 'B-gpe', 'I-org', 'I-geo', 'I-art', 'B-art', 'B-eve', 'I-gpe', 'I-nat', 'B-nat', 'B-per', 'O', 'B-tim'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create CRF model\n",
        "\n",
        "# Training takes time, wait for a few minutes\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    pos_tags = nltk.pos_tag(sent)\n",
        "    word_pos = pos_tags[i][1]\n",
        "    stem = PorterStemmer().stem(word.lower())\n",
        "\n",
        "    features = {\n",
        "        # Word features\n",
        "        'word': word.lower(),\n",
        "        'stem': stem,\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'word.length': len(word),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word[:3]': word[:3],\n",
        "        'word[:2]': word[:2],\n",
        "\n",
        "        # POS features\n",
        "        'pos': word_pos,\n",
        "        'pos[:2]': word_pos[:2],\n",
        "\n",
        "        # Shape features\n",
        "        'word.shape': ''.join(['X' if c.isupper() else 'x' if c.islower()\n",
        "                             else 'd' if c.isdigit() else c for c in word]),\n",
        "\n",
        "        # Context features\n",
        "        'pos-1': pos_tags[i-1][1] if i > 0 else '<START>',\n",
        "        'pos+1': pos_tags[i+1][1] if i < len(sent)-1 else '<END>',\n",
        "        'word-1': sent[i-1].lower() if i > 0 else '<START>',\n",
        "        'word+1': sent[i+1].lower() if i < len(sent)-1 else '<END>',\n",
        "    }\n",
        "\n",
        "    # Add prefix/suffix patterns\n",
        "    for n in range(1, 4):\n",
        "        if len(word) >= n:\n",
        "            features[f'prefix_{n}'] = word[:n]\n",
        "            features[f'suffix_{n}'] = word[-n:]\n",
        "\n",
        "\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "X_train = [sent2features(s) for s in df['Sentence']]\n",
        "y_train = df['NER Tag']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iejL-bpNkn7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter space\n",
        "\n",
        "\n",
        "params = {\n",
        "    'algorithm': ['lbfgs'],\n",
        "    'c1': [ 0.1, 1.0],\n",
        "    'c2': [ 0.1, 1.0],\n",
        "    'max_iterations': [100],\n",
        "    'all_possible_transitions': [True]\n",
        "}\n",
        "\n",
        "crf = CRF()\n",
        "gs = GridSearchCV(crf, params, cv=2, verbose=1, n_jobs=-1)\n",
        "gs.fit(X_train, y_train)  # Fit first\n",
        "print(\"Best parameters:\", gs.best_params_)  # Then print best params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kIMP9cmJ5G3",
        "outputId": "132efd1f-40f0-4ede-e102-c843e302c6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "Best parameters: {'algorithm': 'lbfgs', 'all_possible_transitions': True, 'c1': 0.1, 'c2': 1.0, 'max_iterations': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Get the best model from GridSearch\n",
        "best_crf = gs.best_estimator_\n",
        "\n",
        "# Save the model to a file\n",
        "model_filename = 'crf_model.joblib'\n",
        "joblib.dump(best_crf, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJuhZ4uhZ-0o",
        "outputId": "be2cdbf1-cb81-436d-f20d-e7729a6078dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to crf_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('crf_model.joblib')\n",
        "\n",
        "\n",
        "X_test = [sent2features(s) for s in test['Sentence']]\n",
        "\n",
        "# Make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'NER Tag': [str(pred) for pred in y_pred]\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "kyTG7RPbl-F3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}